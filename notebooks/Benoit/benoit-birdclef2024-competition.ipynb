{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":70203,"databundleVersionId":8068726,"sourceType":"competition"}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"kaggle = True\nsubmission = True # change to True before submitting","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:04:47.677408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Imports","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\n# from tqdm.notebook import tqdm # loading bar\nfrom tqdm import tqdm # loading bar\n\nimport librosa\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.ensemble import RandomForestClassifier","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"if kaggle:\n    DATA_DIR = '../input/birdclef-2024/'\nelse:\n    DATA_DIR = \"../../data/raw\"\n    \nTRAIN_AUDIO_DIR = os.path.join(DATA_DIR, \"train_audio/\")\nTEST_AUDIO_DIR = os.path.join(DATA_DIR,\"test_soundscapes/\")\nUNLABELED_AUDIO_DIR = os.path.join(DATA_DIR,\"unlabeled_soundscapes/\")\n\ntrain_csv_path = os.path.join(DATA_DIR, \"train_metadata.csv\")\nsample_submision_path = os.path.join(DATA_DIR, \"sample_submission.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.read_csv(train_csv_path)\n\n# Add complete filepath\ntrain_df['filepath'] = train_df.apply(lambda row: os.path.join(TRAIN_AUDIO_DIR, row['filename']), axis=1)\n\n# Filter out large files\ntrain_df['filesize'] = train_df.apply(lambda row: os.path.getsize(row['filepath']), axis=1)\ntrain_df = train_df[train_df['filesize'] < 1e6]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"traib_subset_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_state = 43\n\n# Define the number of classes to keep\n\nif submission:\n    num_classes_to_keep = train_df['primary_label'].nunique()\n    # Define the fraction of data to keep for classes with more labels\n    fraction_to_keep = 0.1\nelse:\n    num_classes_to_keep = 100\n    fraction_to_keep = 0.05\n\n# Calculate the minimum number of instances to keep for classes with fewer labels\nmin_count = 50\n\n# Calculate weights to balance the classes\nclass_weights = train_df['primary_label'].value_counts()\n\n# Select the top classes to keep based on their frequencies\ntop_classes = class_weights.head(num_classes_to_keep).index.tolist()\n\n# Initialize an empty DataFrame to store the sampled subset\ntrain_subset_df = pd.DataFrame()\n\n# Iterate over each class\nfor label, count in class_weights.items():\n    # Check if the class is in the top classes to keep\n    if label in top_classes:\n        # Check if the class has fewer labels than the minimum count\n        if count < min_count:\n            # Keep all instances for classes with fewer labels\n            subset = train_df[train_df['primary_label'] == label]\n        else:\n            # Randomly sample a fraction for classes with more labels\n            fraction = min(fraction_to_keep, min_count / count)  # Adjust fraction if necessary\n            subset = train_df[train_df['primary_label'] == label].sample(frac=fraction, random_state=random_state)\n        # Append the subset to the final DataFrame\n        train_subset_df = pd.concat([train_subset_df, subset])\n\n# Shuffle the final DataFrame to mix the classes\ntrain_subset_df = train_subset_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_subset_df['primary_label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if submission: # No train val split\n    X_train_files = train_subset_df.filepath\n    y_train = train_train_df.primary_label\nelse:\n    # Train val split\n    train_train_df, val_df = train_test_split(train_subset_df, test_size=0.3, stratify = train_subset_df.primary_label, random_state=random_state) \n    X_train_files = train_train_df.filepath\n    X_val_files = val_df.filepath\n\n    y_train = train_train_df.primary_label\n    y_val = val_df.primary_label","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Features","metadata":{}},{"cell_type":"code","source":"def extract_features(audio_data, sample_rate=32000, mfcc=True, chroma=True, mel=True):\n    result = np.array([])\n    if mfcc: # Mel-Frequency Cepstral Coefficients\n        mfccs = np.mean(librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40).T, axis=0)\n        result = np.hstack((result, mfccs))\n    if chroma:\n        chroma = np.mean(librosa.feature.chroma_stft(y=audio_data, sr=sample_rate).T,axis=0)\n        result = np.hstack((result, chroma))\n    if mel:\n        mel = np.mean(librosa.feature.melspectrogram(y=audio_data, sr=sample_rate).T,axis=0)\n        result = np.hstack((result, mel))\n    \n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_features_filepaths(X_files, sample_rate=32000):\n    features = []\n    \n    for filepath in tqdm(X_files, desc='Processing files', total=len(X_files)):\n        # Process data with tqdm\n        audio_data, _ = librosa.load(filepath, sr=sample_rate)\n        audio_features = extract_features(audio_data, sample_rate)\n\n        # Append features and label\n        features.append(audio_features)\n            \n    X = np.array(features)  \n    \n    return X \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = extract_features_filepaths(X_train_files)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_encoder = LabelEncoder()\ny_train_encoded = label_encoder.fit_transform(y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#X_val = extract_features_filepaths(X_val_files)\n\n# y_val_encoded = label_encoder.transform(y_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model","metadata":{}},{"cell_type":"code","source":"# Train the classifier with the best parameters\nbest_classifier = RandomForestClassifier(max_depth=20, min_samples_leaf=2, n_estimators=300, random_state=42)\nbest_classifier.fit(X_train, y_train_encoded)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_val_pred_proba = best_classifier.predict_proba(X_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing and submission","metadata":{}},{"cell_type":"code","source":"def extract_numbers(row_id):\n    parts = row_id.split('_')\n    return parts[1]\n\n# First, load list of audio files by parsing the test_soundscape folder.\n\nif submission:\n    test_df = pd.read_csv(sample_submision_path)\n    test_df['numbers'] = test_df['row_id'].apply(extract_numbers)\n    file_list = test_df['numbers'].unique().tolist()\n    print('Number of test soundscapes:', len(file_list))\n    \nelse:\n    file_list = ['1000170626']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to split audio file into chunks of given duration\ndef split_audio(path, duration, sr):\n    sig, rate = librosa.load(path, sr=sr)\n    chunk_size = duration * rate\n    chunks = [sig[i:i+chunk_size] for i in range(0, len(sig), chunk_size)]\n    return chunks","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is where we will store our results\n\npred = {'row_id': [], 'proba': []}\n\n# Process audio files and make predictions\nfor afile in file_list:\n    \n    if submission:\n        filename = 'soundscapes_' + afile + '.ogg'\n        path = os.path.join(TEST_AUDIO_DIR, filename)\n    else:\n        filename = afile + '.ogg'\n        path = os.path.join(UNLABELED_AUDIO_DIR, filename)\n        \n    \n    # Split audio file into 5-second chunks\n    audio_chunks = split_audio(path, duration=5, sr=32000)\n    \n    # Assign the row_id which we need to do for each chunk\n    for i, chunk in enumerate(audio_chunks):\n        chunk_end_time = (i + 1) * 5\n        row_id = f\"soundscape_{afile}_chunk{chunk_end_time}\"\n        pred['row_id'].append(row_id)\n        \n        X_test = extract_features(chunk)  # Assuming you have a function to extract features from audio data\n        X_test = np.expand_dims(X_test, 0)\n\n        y_pred_proba = best_classifier.predict_proba(X_test)\n        pred['proba'].append(y_pred_proba[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert the 'proba' array into a DataFrame\nresults = pd.DataFrame(pred['proba'], columns=label_encoder.classes_)\n\n# Add the 'row_id' column to the DataFrame\nresults['row_id'] = pred['row_id']\n\n# Reorder the columns so that 'row_id' comes first\nresults = results[['row_id'] + list(label_encoder.classes_)]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Convert our results to csv\nresults.to_csv(\"submission.csv\", index=False)  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}