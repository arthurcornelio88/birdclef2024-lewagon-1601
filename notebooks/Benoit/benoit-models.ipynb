{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# import pyogg\n",
    "import librosa\n",
    "\n",
    "from tqdm import tqdm # loading bar\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Kaggle.\n"
     ]
    }
   ],
   "source": [
    "# Check if running on Kaggle\n",
    "kaggle = ('KAGGLE_KERNEL_RUN_TYPE' in os.environ)\n",
    "if kaggle:\n",
    "    # Code specific to Kaggle\n",
    "    print(\"Running on Kaggle!\")\n",
    "else:\n",
    "    print(\"Not running on Kaggle.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numbers(filename):\n",
    "    filename = filename.split('.')[0] # remove extension\n",
    "\n",
    "    split = filename.split('_')\n",
    "\n",
    "    if len(split) > 1:\n",
    "        return split[1]\n",
    "    elif len(split) == 1:\n",
    "        return split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory used for testing: ../../data/raw/test_soundscapes/\n",
      "Number of test files: 1\n"
     ]
    }
   ],
   "source": [
    "if kaggle:\n",
    "    DATA_DIR = '../input/birdclef-2024/'\n",
    "    OUTPUT_DIR = '/kaggle/working/'\n",
    "else: # local work\n",
    "    DATA_DIR = \"../../data/raw/\" \n",
    "    OUTPUT_DIR = \"../../data/processed/\"\n",
    "\n",
    "TRAIN_AUDIO_DIR = os.path.join(DATA_DIR, \"train_audio/\")\n",
    "\n",
    "train_csv_path = os.path.join(DATA_DIR, \"train_metadata.csv\")\n",
    "\n",
    "# Testing\n",
    "TEST_AUDIO_DIR = os.path.join(DATA_DIR,\"test_soundscapes/\")\n",
    "\n",
    "# Load list of audio files by parsing the test_soundscape folder\n",
    "test_file_list = sorted(os.listdir(TEST_AUDIO_DIR))\n",
    "test_file_list = [file for file in test_file_list if file.endswith('.ogg')]  # Filter only ogg files\n",
    "\n",
    "if len(test_file_list) == 0:   # replace test dir by unlabeled dir for testing\n",
    "    TEST_AUDIO_DIR = os.path.join(DATA_DIR, \"unlabeled_soundscapes/\")\n",
    "    test_file_list = sorted(os.listdir(TEST_AUDIO_DIR))\n",
    "    test_file_list = [file for file in test_file_list if file.endswith('.ogg')]  # Filter only ogg files\n",
    "    test_file_list = test_file_list[:5]  # Take only 5 elements to speed up debugging\n",
    "\n",
    "test_number_list = [extract_numbers(file) for file in test_file_list]\n",
    "\n",
    "print(f'Directory used for testing: {TEST_AUDIO_DIR}')\n",
    "print(f'Number of test files: {len(test_file_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_csv_path)\n",
    "\n",
    "# Add complete filepath\n",
    "train_df['filepath'] = train_df.apply(lambda row: os.path.join(TRAIN_AUDIO_DIR, row['filename']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 43\n",
    "\n",
    "# Define the number of classes to keep\n",
    "num_classes_to_keep = 100\n",
    "\n",
    "# Define the fraction of data to keep for classes with more labels\n",
    "fraction_to_keep = 0.05\n",
    "\n",
    "# Calculate the minimum number of instances to keep for classes with fewer labels\n",
    "min_count = 50\n",
    "\n",
    "# Calculate weights to balance the classes\n",
    "class_weights = train_df['primary_label'].value_counts()\n",
    "\n",
    "# Select the top classes to keep based on their frequencies\n",
    "top_classes = class_weights.head(num_classes_to_keep).index.tolist()\n",
    "\n",
    "# Initialize an empty DataFrame to store the sampled subset\n",
    "train_subset_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over each class\n",
    "for label, count in class_weights.items():\n",
    "    # Check if the class is in the top classes to keep\n",
    "    if label in top_classes:\n",
    "        # Check if the class has fewer labels than the minimum count\n",
    "        if count < min_count:\n",
    "            # Keep all instances for classes with fewer labels\n",
    "            subset = train_df[train_df['primary_label'] == label]\n",
    "        else:\n",
    "            # Randomly sample a fraction for classes with more labels\n",
    "            fraction = min(fraction_to_keep, min_count / count)  # Adjust fraction if necessary\n",
    "            subset = train_df[train_df['primary_label'] == label].sample(frac=fraction, random_state=random_state)\n",
    "        # Append the subset to the final DataFrame\n",
    "        train_subset_df = pd.concat([train_subset_df, subset])\n",
    "\n",
    "# Shuffle the final DataFrame to mix the classes\n",
    "train_subset_df = train_subset_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "primary_label\n",
       "blrwar1    25\n",
       "commoo3    25\n",
       "grywag     25\n",
       "lirplo     25\n",
       "comgre     25\n",
       "           ..\n",
       "pursun3     3\n",
       "grefla1     3\n",
       "indpit1     3\n",
       "inbrob1     3\n",
       "insbab1     3\n",
       "Name: count, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subset_df.primary_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train val split\n",
    "train_train_df, val_df = train_test_split(train_subset_df, test_size=0.3, stratify = train_subset_df.primary_label, random_state=random_state) \n",
    "X_train_files = train_train_df.filepath\n",
    "X_val_files = val_df.filepath\n",
    "\n",
    "y_train = train_train_df.primary_label\n",
    "y_val = val_df.primary_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "primary_label\n",
       "bcnher     18\n",
       "zitcis1    18\n",
       "graher1    18\n",
       "comsan     17\n",
       "woosan     17\n",
       "           ..\n",
       "whrmun      2\n",
       "indpit1     2\n",
       "grefla1     2\n",
       "whcbar1     2\n",
       "rossta2     2\n",
       "Name: count, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "primary_label\n",
       "barswa     8\n",
       "blrwar1    8\n",
       "bkwsti     8\n",
       "lirplo     8\n",
       "eurcoo     8\n",
       "          ..\n",
       "shikra1    1\n",
       "grbeat1    1\n",
       "compea     1\n",
       "ashpri1    1\n",
       "whbwoo2    1\n",
       "Name: count, Length: 100, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_val_encoded = label_encoder.transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MFCC (Mel-Frequency Cepstral Coefficients)**\n",
    "\n",
    "MFCCs are a feature widely used in audio and speech processing. They represent the short-term power spectrum of a sound. The process to calculate MFCCs involves several steps:\n",
    "\n",
    "- Frame the Signal: The audio signal is divided into short overlapping frames.\n",
    "- Apply Windowing: Each frame is windowed, typically with a Hamming window, to minimize spectral leakage.\n",
    "- Calculate the Discrete Fourier Transform (DFT): The Fast Fourier Transform (FFT) is applied to each windowed frame to convert the audio signal from the time domain to the frequency domain.\n",
    "- Mel Filtering: Mel filtering is applied to the power spectrum to convert the linear frequency scale to the mel scale, which approximates the human auditory system's response to different frequencies.\n",
    "- Take the Logarithm: The logarithm of the mel-filterbank energies is taken to mimic the human perception of sound intensity.\n",
    "- Apply Discrete Cosine Transform (DCT): Finally, the Discrete Cosine Transform is applied to the mel-log spectrum to decorrelate the features and obtain the MFCCs.\n",
    "- MFCCs capture important spectral characteristics of the audio signal and are commonly used as features for tasks like speech recognition, music genre classification, and audio classification.\n",
    "\n",
    "**Chroma Features**\n",
    "\n",
    "Chroma features represent the distribution of energy in different pitch classes (i.e., musical notes) within an audio signal. Chroma features are particularly useful for tasks involving music analysis and classification, such as genre classification, chord recognition, and instrument recognition.\n",
    "\n",
    "The process to compute chroma features involves the following steps:\n",
    "\n",
    "- Frame the Signal: Similar to MFCCs, the audio signal is divided into short overlapping frames.\n",
    "- Apply Windowing: Each frame is windowed.\n",
    "- Calculate the Short-Time Fourier Transform (STFT): The STFT is applied to each windowed frame to obtain the magnitude spectrum.\n",
    "- Map Frequencies to Chroma: The frequency spectrum is mapped onto the 12 chroma bands corresponding to the 12 pitch classes (C, C#, D, D#, E, F, F#, G, G#, A, A#, B).\n",
    "- Sum Across Octaves: Chroma features are usually computed by summing the energy within each chroma band across octaves.\n",
    "\n",
    "Chroma features are robust to changes in pitch and timbre and are commonly used in music information retrieval tasks.\n",
    "\n",
    "**Mel Spectrogram**\n",
    "\n",
    "A Mel spectrogram is a spectrogram where the frequencies are converted to the mel scale, similar to the mel filtering step in MFCC computation. It represents the distribution of energy in different frequency bands over time. Mel spectrograms are often used as features for audio classification tasks.\n",
    "\n",
    "These features capture different aspects of the audio signal's spectral content and are useful for various audio analysis and classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features1(filepath, mfcc=True, chroma=True, mel=True):\n",
    "    sample_rate = 32000\n",
    "    audio_data, _ = librosa.load(filepath, sr=sample_rate)\n",
    "    \n",
    "    features = []\n",
    "    \n",
    "    if mfcc:\n",
    "        mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40)\n",
    "        features.append(np.mean(mfccs, axis=1))\n",
    "    \n",
    "    if chroma:\n",
    "        chroma = librosa.feature.chroma_stft(y=audio_data, sr=sample_rate)\n",
    "        features.append(np.mean(chroma, axis=1))\n",
    "    \n",
    "    if mel:\n",
    "        mel = librosa.feature.melspectrogram(y=audio_data, sr=sample_rate)\n",
    "        features.append(np.mean(mel, axis=1))\n",
    "    \n",
    "    return np.concatenate(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_filepaths1(X_files):\n",
    "    features = []\n",
    "\n",
    "    # Wrap the loop with tqdm to add a progress bar\n",
    "    for filepath in tqdm(X_files, desc='Processing files', total=len(X_files)):\n",
    "\n",
    "        # Extract features\n",
    "        audio_features = extract_features1(filepath)\n",
    "\n",
    "        # Append features and label\n",
    "        features.append(audio_features)\n",
    "\n",
    "    X = np.array(features)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  36%|████████████████████████████████▏                                                        | 283/784 [02:52<05:53,  1.42it/s]/home/bfrisque/.pyenv/versions/3.10.6/envs/birdclef2024-lewagon-1601/lib/python3.10/site-packages/librosa/core/pitch.py:101: UserWarning: Trying to estimate tuning from empty frequency set.\n",
      "  return pitch_tuning(\n",
      "Processing files:  65%|██████████████████████████████████████████████████████████                               | 512/784 [04:57<02:44,  1.65it/s]"
     ]
    }
   ],
   "source": [
    "X_train1 = extract_features_filepaths1(X_train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val1 = extract_features_filepaths1(X_val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train1.T);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: improved feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features2(filepath, mfcc=True, chroma=True, mel=True, contrast=True, centroid=True, zero_crossing=True, rms_energy=True, threshold=0.1):\n",
    "    sample_rate = 32000\n",
    "    audio_data, _ = librosa.load(filepath, sr=sample_rate)\n",
    "    \n",
    "    features = []\n",
    "    bird_song_segments = []\n",
    "    \n",
    "    if rms_energy:\n",
    "        rms = librosa.feature.rms(y=audio_data)\n",
    "        rms_mean = np.mean(rms)\n",
    "        rms_threshold = threshold * rms_mean\n",
    "        \n",
    "        # Identify segments where RMS energy exceeds the threshold\n",
    "        segments = librosa.effects.split(y=audio_data, top_db=rms_threshold)\n",
    "        \n",
    "        for segment in segments:\n",
    "            bird_song_segments.extend(range(segment[0], segment[1] + 1))\n",
    "    \n",
    "    if mfcc:\n",
    "        mfccs = librosa.feature.mfcc(y=audio_data[bird_song_segments], sr=sample_rate, n_mfcc=40)\n",
    "        features.append(np.mean(mfccs, axis=1))\n",
    "    \n",
    "    if chroma:\n",
    "        chroma = librosa.feature.chroma_stft(y=audio_data[bird_song_segments], sr=sample_rate)\n",
    "        features.append(np.mean(chroma, axis=1))\n",
    "    \n",
    "    if mel:\n",
    "        mel = librosa.feature.melspectrogram(y=audio_data[bird_song_segments], sr=sample_rate)\n",
    "        features.append(np.mean(mel, axis=1))\n",
    "    \n",
    "    if contrast:\n",
    "        contrast = librosa.feature.spectral_contrast(y=audio_data[bird_song_segments], sr=sample_rate)\n",
    "        features.append(np.mean(contrast, axis=1))\n",
    "    \n",
    "    if centroid:\n",
    "        centroid = librosa.feature.spectral_centroid(y=audio_data[bird_song_segments], sr=sample_rate)\n",
    "        features.append(np.mean(centroid, axis=1))  # Modify this line\n",
    "    \n",
    "    if zero_crossing:\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(y=audio_data[bird_song_segments])\n",
    "        features.append(np.mean(zero_crossing_rate, axis=1))  # Modify this line\n",
    "    \n",
    "    return np.concatenate(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_filepaths2(X_files):\n",
    "    features = []\n",
    "\n",
    "    # Wrap the loop with tqdm to add a progress bar\n",
    "    for filepath in tqdm(X_files, desc='Processing files', total=len(X_files)):\n",
    "\n",
    "        # Extract features\n",
    "        audio_features = extract_features2(filepath)\n",
    "\n",
    "        # Append features and label\n",
    "        features.append(audio_features)\n",
    "\n",
    "    X = np.array(features)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = extract_features_filepaths2(X_train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val2 = extract_features_filepaths2(X_val_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 : Features for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features3(filepath, max_length=3000):\n",
    "    sample_rate = 32000\n",
    "    audio_data, _ = librosa.load(filepath, sr=sample_rate)\n",
    "    mfccs = librosa.feature.mfcc(y=audio_data, sr=sample_rate, n_mfcc=40)\n",
    "    \n",
    "    # Pad or truncate mfccs to ensure a fixed length\n",
    "    if mfccs.shape[1] < max_length:\n",
    "        pad_width = max_length - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "    elif mfccs.shape[1] > max_length:\n",
    "        mfccs = mfccs[:, :max_length]\n",
    "    \n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_filepaths3(X_files):\n",
    "    features = []\n",
    "\n",
    "    # Wrap the loop with tqdm to add a progress bar\n",
    "    for filepath in tqdm(X_files, desc='Processing files', total=len(X_files)):\n",
    "\n",
    "        # Extract features\n",
    "        audio_features = extract_features3(filepath)\n",
    "\n",
    "        # Append features and label\n",
    "        features.append(audio_features)\n",
    "\n",
    "    X = np.array(features)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 : Baseline - RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier\n",
    "classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "classifier.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred_encoded = classifier.predict(X_val)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_val_encoded, y_pred_encoded)\n",
    "print(f\"Accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2 : Random forest with feature encoding 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier\n",
    "classifier_bis = RandomForestClassifier(n_estimators=500, random_state=42, verbose=1, n_jobs=-1)\n",
    "classifier_bis.fit(X_train_bis, y_train_encoded)-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred_encoded_bis = classifier_bis.predict(X_val_bis)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_bis = accuracy_score(y_val_encoded, y_pred_encoded_bis)\n",
    "print(f\"Accuracy: {accuracy_bis:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3 : Deep learning with CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train3 = extract_features_filepaths3(X_train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val3 = extract_features_filepaths3(X_val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding number of channels (1)\n",
    "X_train3 = np.expand_dims(X_train3, -1)\n",
    "X_val3 = np.expand_dims(X_val3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train_encoded)\n",
    "y_val_cat = to_categorical(y_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "# Define CNN architecture\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(input_shape=X_train3.shape[1:], num_classes=num_classes_to_keep)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define the early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Now, you can include the early stopping callback in your model.fit() call\n",
    "history = model.fit(X_train3, y_train_cat, batch_size=32, epochs=100, validation_data=(X_val3, y_val_cat), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a random forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)\n",
    "\n",
    "# Train the classifier with the best parameters\n",
    "best_classifier = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_classifier.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for tuning\n",
    "param_dist= {\n",
    "    'n_estimators': stats.randint(250, 400),\n",
    "    'max_depth': stats.randint(15, 30),\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Create a random forest classifier\n",
    "classifier = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Perform grid search cross-validation\n",
    "random_search = RandomizedSearchCV(estimator=classifier, param_distributions=param_dist, cv=3, n_jobs=-1, verbose=2)\n",
    "random_search.fit(X_train, y_train_encoded)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier with the best parameters\n",
    "best_classifier = RandomForestClassifier(max_depth=20, min_samples_leaf=2, n_estimators=300, random_state=42)\n",
    "best_classifier.fit(X_train, y_train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "y_pred_encoded = best_classifier.predict(X_val)\n",
    "\n",
    "# # Evaluate\n",
    "accuracy = accuracy_score(y_val_encoded, y_pred_encoded)\n",
    "print(f\"Accuracy: {accuracy:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_train = xgb.DMatrix(X_train, label=y_train_encoded)\n",
    "xg_val = xgb.DMatrix(X_val, label=y_val_encoded)\n",
    "\n",
    "# setup parameters for xgboost\n",
    "param = {}\n",
    "# use softmax multi-class classification\n",
    "param['objective'] = 'multi:softmax'\n",
    "# scale weight of positive examples\n",
    "param['eta'] = 0.1\n",
    "param['max_depth'] = 10\n",
    "param['nthread'] = 4\n",
    "param['num_class'] = y_train.nunique()\n",
    "\n",
    "\n",
    "# Specify which dataset and which metric should be used for early stopping.\n",
    "early_stop = xgb.callback.EarlyStopping(rounds=5,\n",
    "                                        save_best=True,\n",
    "                                        data_name='validation')\n",
    "\n",
    "watchlist = [(xg_train, 'train'), (xg_val, 'validation')]\n",
    "\n",
    "xgb_classifier = xgb.train(param, xg_train, num_boost_round=500, evals=watchlist, callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred_encoded = xgb_classifier.predict(dval).astype(np.int32)\n",
    "\n",
    "# Decode the predicted labels\n",
    "y_val_pred = label_encoder.inverse_transform(y_val_pred_encoded)\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Accuracy: {accuracy:.1%}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8068726,
     "sourceId": 70203,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
